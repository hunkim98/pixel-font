{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2995987569997363\n",
      "=== epoch:1, train acc:0.164, test acc:0.185 ===\n",
      "train loss:2.2969454462393735\n",
      "train loss:2.2956511525881576\n",
      "train loss:2.288873147051098\n",
      "train loss:2.2779309251562934\n",
      "train loss:2.267524276687751\n",
      "train loss:2.2567504245255194\n",
      "train loss:2.245327551628069\n",
      "train loss:2.2222254667219357\n",
      "train loss:2.19957506458792\n",
      "train loss:2.1675540674187643\n",
      "train loss:2.1278920875700083\n",
      "train loss:2.1295355229259583\n",
      "train loss:2.060723156607638\n",
      "train loss:2.039790040638368\n",
      "train loss:1.9534604539724196\n",
      "train loss:1.8808545023571837\n",
      "train loss:1.8478042552573957\n",
      "train loss:1.766464869373744\n",
      "train loss:1.6592142087874735\n",
      "train loss:1.6094918840207126\n",
      "train loss:1.4997054144252653\n",
      "train loss:1.4438719372670317\n",
      "train loss:1.297192894392198\n",
      "train loss:1.3173096181058812\n",
      "train loss:1.2316093130561883\n",
      "train loss:1.0979927932735933\n",
      "train loss:0.9867645336755078\n",
      "train loss:1.0438054504009373\n",
      "train loss:0.8888943755281556\n",
      "train loss:0.8671129638516178\n",
      "train loss:0.8583773154638722\n",
      "train loss:0.8381527537659109\n",
      "train loss:0.8521328483884123\n",
      "train loss:0.922678937114239\n",
      "train loss:0.8430264301161087\n",
      "train loss:0.6831960773170751\n",
      "train loss:0.6140016520117499\n",
      "train loss:0.6277380416715043\n",
      "train loss:0.6112521257789924\n",
      "train loss:0.7390453968092173\n",
      "train loss:0.6455553192907988\n",
      "train loss:0.6275432218600275\n",
      "train loss:0.66493015151617\n",
      "train loss:0.8468540068831057\n",
      "train loss:0.5717763905478779\n",
      "train loss:0.712556908679737\n",
      "train loss:0.8243918837984633\n",
      "train loss:0.5424423326270591\n",
      "train loss:0.6649171110608241\n",
      "train loss:0.5855957483544971\n",
      "train loss:0.6284970495222759\n",
      "train loss:0.6677716988815824\n",
      "train loss:0.5833772837623009\n",
      "train loss:0.5066461685594313\n",
      "train loss:0.48926105692421223\n",
      "train loss:0.4052536651084678\n",
      "train loss:0.5531221090460434\n",
      "train loss:0.5072143311863322\n",
      "train loss:0.5901458064908826\n",
      "train loss:0.5964582957206669\n",
      "train loss:0.5863297452576854\n",
      "train loss:0.3278395589224635\n",
      "train loss:0.5757211797913772\n",
      "train loss:0.5852008300228447\n",
      "train loss:0.5838102557624975\n",
      "train loss:0.4523168844103534\n",
      "train loss:0.3117855419934115\n",
      "train loss:0.5500633056606945\n",
      "train loss:0.5143277399070203\n",
      "train loss:0.6976585739533799\n",
      "train loss:0.47423274256253\n",
      "train loss:0.5276473795520156\n",
      "train loss:0.3375835705333469\n",
      "train loss:0.2555317952996239\n",
      "train loss:0.53441981853514\n",
      "train loss:0.44795432240452837\n",
      "train loss:0.37427023443079244\n",
      "train loss:0.5052612288695478\n",
      "train loss:0.41628905840370356\n",
      "train loss:0.25065398575497766\n",
      "train loss:0.3670832461136385\n",
      "train loss:0.4112933834377077\n",
      "train loss:0.4442293325757851\n",
      "train loss:0.29873793037896407\n",
      "train loss:0.38397045538101937\n",
      "train loss:0.5205304694274887\n",
      "train loss:0.3532970647947105\n",
      "train loss:0.5016207407426049\n",
      "train loss:0.4232225781914016\n",
      "train loss:0.341164787328874\n",
      "train loss:0.5793632702164224\n",
      "train loss:0.4145655911562669\n",
      "train loss:0.29660543689592855\n",
      "train loss:0.469116174980797\n",
      "train loss:0.5225721590464777\n",
      "train loss:0.47703350230274716\n",
      "train loss:0.44268629057802433\n",
      "train loss:0.40545378735967974\n",
      "train loss:0.3378766553442389\n",
      "train loss:0.6784249066077783\n",
      "train loss:0.32418521286816904\n",
      "train loss:0.455084531053234\n",
      "train loss:0.355577285739065\n",
      "train loss:0.4047589780522436\n",
      "train loss:0.2575505690400609\n",
      "train loss:0.2530321239871293\n",
      "train loss:0.462344527999873\n",
      "train loss:0.2589802806840613\n",
      "train loss:0.2376328511205445\n",
      "train loss:0.448387244866508\n",
      "train loss:0.5075674475001964\n",
      "train loss:0.3935641198979616\n",
      "train loss:0.34128783407427166\n",
      "train loss:0.4454906690011749\n",
      "train loss:0.37513913803905397\n",
      "train loss:0.3437897476336552\n",
      "train loss:0.4695333379843418\n",
      "train loss:0.35103890347091843\n",
      "train loss:0.41569758376853677\n",
      "train loss:0.3246707951511669\n",
      "train loss:0.34645305653379893\n",
      "train loss:0.3606431598244056\n",
      "train loss:0.26250055909497066\n",
      "train loss:0.2608190319323873\n",
      "train loss:0.31176940043094775\n",
      "train loss:0.362493643972431\n",
      "train loss:0.3387142200346307\n",
      "train loss:0.29356045600909936\n",
      "train loss:0.5551744811538616\n",
      "train loss:0.414522710780022\n",
      "train loss:0.3782284931432873\n",
      "train loss:0.3263119082046723\n",
      "train loss:0.28358402961039747\n",
      "train loss:0.41381698681171736\n",
      "train loss:0.5770467981986366\n",
      "train loss:0.4249406167955968\n",
      "train loss:0.3810490465304694\n",
      "train loss:0.43060197279445916\n",
      "train loss:0.4370471749256392\n",
      "train loss:0.3631783881216515\n",
      "train loss:0.29477728379736445\n",
      "train loss:0.23812412562473695\n",
      "train loss:0.3394954768647167\n",
      "train loss:0.3452698690121219\n",
      "train loss:0.33747694589040345\n",
      "train loss:0.3116173502724562\n",
      "train loss:0.3744718786182426\n",
      "train loss:0.44866153968732797\n",
      "train loss:0.36474945759962657\n",
      "train loss:0.3344601015948915\n",
      "train loss:0.45802908529698494\n",
      "train loss:0.4578771605245645\n",
      "train loss:0.2820901590995848\n",
      "train loss:0.3876485961686263\n",
      "train loss:0.32922371123433125\n",
      "train loss:0.330656739422282\n",
      "train loss:0.41315352393359867\n",
      "train loss:0.3908478300491706\n",
      "train loss:0.5333415631604619\n",
      "train loss:0.25896465011030445\n",
      "train loss:0.4188253173371168\n",
      "train loss:0.3138909388268275\n",
      "train loss:0.2796420211217893\n",
      "train loss:0.22512335804159964\n",
      "train loss:0.2128459629638449\n",
      "train loss:0.2220856248454006\n",
      "train loss:0.2840311679139746\n",
      "train loss:0.4600682176024373\n",
      "train loss:0.3658173337582209\n",
      "train loss:0.35725974090369755\n",
      "train loss:0.2810813513767676\n",
      "train loss:0.20952351435401848\n",
      "train loss:0.43559802005071546\n",
      "train loss:0.25717080144297866\n",
      "train loss:0.33124676856840696\n",
      "train loss:0.2878444690415343\n",
      "train loss:0.28490539359137057\n",
      "train loss:0.2850025339748514\n",
      "train loss:0.2128571336074166\n",
      "train loss:0.24607687721434562\n",
      "train loss:0.4042724469320826\n",
      "train loss:0.2938046965552998\n",
      "train loss:0.2828509625869924\n",
      "train loss:0.3944805567731538\n",
      "train loss:0.31254825158337834\n",
      "train loss:0.34415999563029503\n",
      "train loss:0.22225652168190327\n",
      "train loss:0.23447812533382165\n",
      "train loss:0.2496700115014984\n",
      "train loss:0.28897355974256095\n",
      "train loss:0.22263549743302513\n",
      "train loss:0.3177840776209339\n",
      "train loss:0.31596776730837833\n",
      "train loss:0.3758849510602021\n",
      "train loss:0.24774039290473485\n",
      "train loss:0.26258399426957274\n",
      "train loss:0.38906226631250285\n",
      "train loss:0.25148872528924615\n",
      "train loss:0.2272007135480269\n",
      "train loss:0.16149446299270898\n",
      "train loss:0.418575727957009\n",
      "train loss:0.336203529817652\n",
      "train loss:0.16983898077759213\n",
      "train loss:0.22683223840498296\n",
      "train loss:0.3564824319388601\n",
      "train loss:0.16072465019334292\n",
      "train loss:0.22005814277199487\n",
      "train loss:0.33573762799623763\n",
      "train loss:0.39069247150411085\n",
      "train loss:0.30816161258591906\n",
      "train loss:0.32409064972436835\n",
      "train loss:0.24029123226083599\n",
      "train loss:0.1749423044407482\n",
      "train loss:0.38200788293719234\n",
      "train loss:0.3042520141892384\n",
      "train loss:0.2227120820816449\n",
      "train loss:0.31959292838137077\n",
      "train loss:0.2213066388624302\n",
      "train loss:0.22919191772298958\n",
      "train loss:0.2844445902843773\n",
      "train loss:0.24735978383736504\n",
      "train loss:0.14810889610954742\n",
      "train loss:0.29547538827392783\n",
      "train loss:0.2857940190759123\n",
      "train loss:0.17978295169572928\n",
      "train loss:0.09934461213930748\n",
      "train loss:0.527693691787658\n",
      "train loss:0.2161195803669117\n",
      "train loss:0.18586008420535488\n",
      "train loss:0.26567989406718917\n",
      "train loss:0.2219420509607663\n",
      "train loss:0.25384907290483283\n",
      "train loss:0.22901924379210697\n",
      "train loss:0.2832250945638771\n",
      "train loss:0.3621718869885644\n",
      "train loss:0.2436614750498397\n",
      "train loss:0.23037764658900828\n",
      "train loss:0.20426726039177456\n",
      "train loss:0.24017461363657816\n",
      "train loss:0.18467414466657137\n",
      "train loss:0.3580782826449909\n",
      "train loss:0.20609443443267053\n",
      "train loss:0.24545267101552856\n",
      "train loss:0.6806762915486619\n",
      "train loss:0.28890669546182823\n",
      "train loss:0.2996679401451507\n",
      "train loss:0.15295772284177314\n",
      "train loss:0.2682148005142504\n",
      "train loss:0.2666244924018582\n",
      "train loss:0.11847598621990071\n",
      "train loss:0.23300802955693756\n",
      "train loss:0.22912250018752853\n",
      "train loss:0.31300145883353586\n",
      "train loss:0.1778719705431817\n",
      "train loss:0.19878729794993055\n",
      "train loss:0.2912904485698025\n",
      "train loss:0.36152352581152497\n",
      "train loss:0.13175873582118292\n",
      "train loss:0.1568591363269118\n",
      "train loss:0.214263662345269\n",
      "train loss:0.3238145830444417\n",
      "train loss:0.1744261584756221\n",
      "train loss:0.1503024037074541\n",
      "train loss:0.3320154640759967\n",
      "train loss:0.10180140544737613\n",
      "train loss:0.14962137169463935\n",
      "train loss:0.3037813459237645\n",
      "train loss:0.17287878703577053\n",
      "train loss:0.25769296978321815\n",
      "train loss:0.18690500044551459\n",
      "train loss:0.25595867025728725\n",
      "train loss:0.30376881161304614\n",
      "train loss:0.28003694605660184\n",
      "train loss:0.36208977221554145\n",
      "train loss:0.24039521869839103\n",
      "train loss:0.2692907196392806\n",
      "train loss:0.18921894276327988\n",
      "train loss:0.30463193154515855\n",
      "train loss:0.2618089697616971\n",
      "train loss:0.1767036788407032\n",
      "train loss:0.19345954289388456\n",
      "train loss:0.32505395019446526\n",
      "train loss:0.2647118371431223\n",
      "train loss:0.248775943641827\n",
      "train loss:0.2023983769663079\n",
      "train loss:0.25797428759665497\n",
      "train loss:0.23937233404619607\n",
      "train loss:0.24309350009202849\n",
      "train loss:0.18579937380512745\n",
      "train loss:0.20571151411669167\n",
      "train loss:0.21414722029511346\n",
      "train loss:0.22559564488584524\n",
      "train loss:0.19548455086333555\n",
      "train loss:0.16989434323696465\n",
      "train loss:0.18041785307292635\n",
      "train loss:0.19504661523687367\n",
      "train loss:0.24199144582540597\n",
      "train loss:0.13710013756460826\n",
      "train loss:0.24417699471466545\n",
      "train loss:0.1941316627341182\n",
      "train loss:0.138810418117063\n",
      "train loss:0.22612481239471535\n",
      "train loss:0.24655129424441338\n",
      "train loss:0.23424648327693298\n",
      "train loss:0.14640357757554534\n",
      "train loss:0.17812277494352327\n",
      "train loss:0.1712972579726188\n",
      "train loss:0.1875332057852405\n",
      "train loss:0.23224100517696988\n",
      "train loss:0.27177950160308406\n",
      "train loss:0.2990378523669967\n",
      "train loss:0.25242416915441246\n",
      "train loss:0.319434422495675\n",
      "train loss:0.13565842226283858\n",
      "train loss:0.1572338719427147\n",
      "train loss:0.28644770061777786\n",
      "train loss:0.21197087941876763\n",
      "train loss:0.2400683409043204\n",
      "train loss:0.14013069999901556\n",
      "train loss:0.14344987458486133\n",
      "train loss:0.19957659409687686\n",
      "train loss:0.2129171578553731\n",
      "train loss:0.25164431234967216\n",
      "train loss:0.13352392028196466\n",
      "train loss:0.36620538835901895\n",
      "train loss:0.1524689844130257\n",
      "train loss:0.13808350643496145\n",
      "train loss:0.10553599333487648\n",
      "train loss:0.2398370995915159\n",
      "train loss:0.15249225975501196\n",
      "train loss:0.4991151141622726\n",
      "train loss:0.24585881449839403\n",
      "train loss:0.17931864829258234\n",
      "train loss:0.2188134196584209\n",
      "train loss:0.20802553740082644\n",
      "train loss:0.13621687997377532\n",
      "train loss:0.19643102273373006\n",
      "train loss:0.2693191318015915\n",
      "train loss:0.22277457812691848\n",
      "train loss:0.11544201607734023\n",
      "train loss:0.12444478356164615\n",
      "train loss:0.21085905172176278\n",
      "train loss:0.1666333144821052\n",
      "train loss:0.22411614035838234\n",
      "train loss:0.15859487361707664\n",
      "train loss:0.18507511638178126\n",
      "train loss:0.079689656539432\n",
      "train loss:0.19370104141892436\n",
      "train loss:0.11575171191019626\n",
      "train loss:0.2648550378323789\n",
      "train loss:0.3408147667182274\n",
      "train loss:0.15548786089897085\n",
      "train loss:0.16790636556973698\n",
      "train loss:0.16837712296821328\n",
      "train loss:0.2103179349290382\n",
      "train loss:0.22319813443387745\n",
      "train loss:0.1637294779727231\n",
      "train loss:0.12773314258457447\n",
      "train loss:0.1162287511444123\n",
      "train loss:0.12386532965851481\n",
      "train loss:0.1351169981414539\n",
      "train loss:0.11775554581406983\n",
      "train loss:0.07373197700515076\n",
      "train loss:0.2434555959036492\n",
      "train loss:0.17858618720512137\n",
      "train loss:0.08107302486719219\n",
      "train loss:0.11409076991579208\n",
      "train loss:0.2316513923963817\n",
      "train loss:0.1021892658715597\n",
      "train loss:0.17678447693282437\n",
      "train loss:0.14848938365101558\n",
      "train loss:0.10968468063659813\n",
      "train loss:0.16149136763886981\n",
      "train loss:0.1846653917023685\n",
      "train loss:0.11824344640230548\n",
      "train loss:0.20035257618334593\n",
      "train loss:0.27131370265177057\n",
      "train loss:0.08648836388676676\n",
      "train loss:0.12428266915240593\n",
      "train loss:0.14596599138395547\n",
      "train loss:0.12133764010689772\n",
      "train loss:0.1493790046753678\n",
      "train loss:0.09508220292148288\n",
      "train loss:0.1741750668419993\n",
      "train loss:0.1742102657795026\n",
      "train loss:0.15889240774376756\n",
      "train loss:0.11488161934032297\n",
      "train loss:0.17796101642415024\n",
      "train loss:0.10391895567877796\n",
      "train loss:0.1079664326024676\n",
      "train loss:0.09979378222658675\n",
      "train loss:0.08314810774831864\n",
      "train loss:0.278356841850712\n",
      "train loss:0.12543594629194532\n",
      "train loss:0.18419295058264765\n",
      "train loss:0.15571589460181767\n",
      "train loss:0.18862138958407124\n",
      "train loss:0.2140031652181599\n",
      "train loss:0.11602354179288285\n",
      "train loss:0.14351760383079734\n",
      "train loss:0.329959886834396\n",
      "train loss:0.07298616879141245\n",
      "train loss:0.24251254389355473\n",
      "train loss:0.06358378203446594\n",
      "train loss:0.15019921616586895\n",
      "train loss:0.12307871229192187\n",
      "train loss:0.14103259066829174\n",
      "train loss:0.18728961175963177\n",
      "train loss:0.2651431710340308\n",
      "train loss:0.1099614923731834\n",
      "train loss:0.20721208676904992\n",
      "train loss:0.09957614430621245\n",
      "train loss:0.11732876460211808\n",
      "train loss:0.161041565136431\n",
      "train loss:0.16830307834419395\n",
      "train loss:0.3514018212304004\n",
      "train loss:0.08623568008536321\n",
      "train loss:0.26106625331347844\n",
      "train loss:0.18360541948508347\n",
      "train loss:0.3380369514223163\n",
      "train loss:0.26503639547448615\n",
      "train loss:0.08897890052729095\n",
      "train loss:0.11728405050539034\n",
      "train loss:0.08978416553236433\n",
      "train loss:0.12861713099064104\n",
      "train loss:0.14302109469878346\n",
      "train loss:0.09979403228368765\n",
      "train loss:0.1288908307159465\n",
      "train loss:0.07963878341870212\n",
      "train loss:0.1823867893949488\n",
      "train loss:0.14906005505825692\n",
      "train loss:0.14028699199992753\n",
      "train loss:0.14378651660509448\n",
      "train loss:0.1082858459233368\n",
      "train loss:0.12517201439694237\n",
      "train loss:0.20859894977281815\n",
      "train loss:0.24078284988006848\n",
      "train loss:0.17686862358150837\n",
      "train loss:0.2683877448592476\n",
      "train loss:0.1087267541959448\n",
      "train loss:0.09418728080440983\n",
      "train loss:0.11934025492783723\n",
      "train loss:0.13249603639893473\n",
      "train loss:0.07052943631275439\n",
      "train loss:0.11804013859437766\n",
      "train loss:0.10675558336439636\n",
      "train loss:0.13318461904196502\n",
      "train loss:0.10717734287874746\n",
      "train loss:0.08086660365007582\n",
      "train loss:0.0685948359871895\n",
      "train loss:0.12121612095116233\n",
      "train loss:0.22038151097078157\n",
      "train loss:0.10532004267392102\n",
      "train loss:0.23268913397942473\n",
      "train loss:0.08599429818943292\n",
      "train loss:0.14802300597126236\n",
      "train loss:0.11556753371883294\n",
      "train loss:0.08285167367237839\n",
      "train loss:0.2100222285544986\n",
      "train loss:0.14462143024971255\n",
      "train loss:0.07721370955479621\n",
      "train loss:0.08080400041800458\n",
      "train loss:0.22157027919652225\n",
      "train loss:0.11601455058773563\n",
      "train loss:0.18063076345365608\n",
      "train loss:0.21480956077438804\n",
      "train loss:0.14658497118304997\n",
      "train loss:0.10927487893371177\n",
      "train loss:0.15731711384598346\n",
      "train loss:0.08583588528708745\n",
      "train loss:0.146852879432844\n",
      "train loss:0.16697487303925976\n",
      "train loss:0.10243179153178485\n",
      "train loss:0.239974926405127\n",
      "train loss:0.15111599052879804\n",
      "train loss:0.33858344516778893\n",
      "train loss:0.1636342361938875\n",
      "train loss:0.11202330247475777\n",
      "train loss:0.193632287084428\n",
      "train loss:0.14151274741303196\n",
      "train loss:0.1535912239646949\n",
      "train loss:0.0627131026498828\n",
      "train loss:0.15250989432923848\n",
      "train loss:0.07118846098065088\n",
      "train loss:0.19756152388628703\n",
      "train loss:0.10664979524381513\n",
      "train loss:0.09336450185699235\n",
      "train loss:0.14180800972254276\n",
      "train loss:0.15625988244492522\n",
      "train loss:0.15916693567531426\n",
      "train loss:0.18015062782282348\n",
      "train loss:0.24467413542890654\n",
      "train loss:0.161222215330768\n",
      "train loss:0.12866980188116298\n",
      "train loss:0.08271204441232921\n",
      "train loss:0.16078130211995462\n",
      "train loss:0.20327134358671892\n",
      "train loss:0.10752920852969253\n",
      "train loss:0.05858845743712247\n",
      "train loss:0.10024586842185998\n",
      "train loss:0.08828261490290741\n",
      "train loss:0.1405594134865655\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     16\u001b[0m network \u001b[39m=\u001b[39m SimpleConvNet(input_dim\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m28\u001b[39m,\u001b[39m28\u001b[39m), \n\u001b[1;32m     17\u001b[0m                         conv_param \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mfilter_num\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m30\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfilter_size\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpad\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstride\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m},\n\u001b[1;32m     18\u001b[0m                         hidden_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, output_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, weight_init_std\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m     20\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[1;32m     21\u001b[0m                   epochs\u001b[39m=\u001b[39mmax_epochs, mini_batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m     22\u001b[0m                   optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m'\u001b[39m, optimizer_param\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.001\u001b[39m},\n\u001b[1;32m     23\u001b[0m                   evaluate_sample_num_per_epoch\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     26\u001b[0m \u001b[39m# 매개변수 보존\u001b[39;00m\n\u001b[1;32m     27\u001b[0m network\u001b[39m.\u001b[39msave_params(\u001b[39m\"\u001b[39m\u001b[39mparams.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Github/pixel_font/common/trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter):\n\u001b[0;32m---> 71\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step()\n\u001b[1;32m     73\u001b[0m     test_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39maccuracy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_test, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_test)\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/Github/pixel_font/common/trainer.py:47\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparams, grads)\n\u001b[0;32m---> 47\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork\u001b[39m.\u001b[39;49mloss(x_batch, t_batch)\n\u001b[1;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loss_list\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtrain loss:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(loss))\n",
      "File \u001b[0;32m~/Github/pixel_font/common/conv_net.py:111\u001b[0m, in \u001b[0;36mSimpleConvNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, x, t):\n\u001b[0;32m--> 111\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x)\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_layer\u001b[39m.\u001b[39mforward(y, t)\n",
      "File \u001b[0;32m~/Github/pixel_font/common/conv_net.py:107\u001b[0m, in \u001b[0;36mSimpleConvNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    106\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m--> 107\u001b[0m         x \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Github/pixel_font/common/conv_layers.py:119\u001b[0m, in \u001b[0;36mPooling.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m col \u001b[39m=\u001b[39m col\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool_h\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool_w)\n\u001b[1;32m    118\u001b[0m arg_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(col, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmax(col, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    120\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mreshape(N, out_h, out_w, C)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[39m# we save the index of the maximum value in each column\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# we will use this index to calculate the gradient in the backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/pixel_font/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[1;32m   2693\u001b[0m \u001b[39m@set_module\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2694\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2695\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2696\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2697\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2698\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2810\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[1;32m   2811\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/Github/pixel_font/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.conv_net import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
