{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.conv_net import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 1, 28, 28), (60000,), (10000, 1, 28, 28), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, t_train.shape, x_test.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2999419091716375\n",
      "=== epoch:1, train acc:0.111, test acc:0.13 ===\n",
      "train loss:2.2970393427666607\n",
      "train loss:2.293190641843923\n",
      "train loss:2.2894048944884986\n",
      "train loss:2.2887907306276736\n",
      "train loss:2.2705632437926715\n",
      "train loss:2.2610028408030995\n",
      "train loss:2.2595880089764973\n",
      "train loss:2.236389380244313\n",
      "train loss:2.2233715231126077\n",
      "train loss:2.1961403069430068\n",
      "train loss:2.155519061495578\n",
      "train loss:2.117741932511143\n",
      "train loss:2.098046968778462\n",
      "train loss:2.0035076062201713\n",
      "train loss:1.933019053670001\n",
      "train loss:1.8880411970557998\n",
      "train loss:1.8668463566643365\n",
      "train loss:1.8633633314247944\n",
      "train loss:1.7565034708848208\n",
      "train loss:1.6873007861968017\n",
      "train loss:1.595490100716967\n",
      "train loss:1.4664793515526335\n",
      "train loss:1.4676494152338528\n",
      "train loss:1.4075774449477287\n",
      "train loss:1.3574279418061856\n",
      "train loss:1.2251235224333592\n",
      "train loss:1.2451021188783191\n",
      "train loss:1.04174527563015\n",
      "train loss:1.0300125083706733\n",
      "train loss:0.9089895576322733\n",
      "train loss:0.8941623069125351\n",
      "train loss:0.9218264566391224\n",
      "train loss:0.8560190771072078\n",
      "train loss:0.837644397137436\n",
      "train loss:0.8129369751826248\n",
      "train loss:0.7411065344233971\n",
      "train loss:0.7141148591648665\n",
      "train loss:0.7024073859904424\n",
      "train loss:0.5391255572777935\n",
      "train loss:0.5615795479342474\n",
      "train loss:0.6001417519236076\n",
      "train loss:0.6566006956333189\n",
      "train loss:0.5190148849838621\n",
      "train loss:0.6110526351407033\n",
      "train loss:0.6663580386624065\n",
      "train loss:0.5494639533420128\n",
      "train loss:0.5774854647988165\n",
      "train loss:0.7504742720622236\n",
      "train loss:0.46237249393176577\n",
      "train loss:0.5305200902062135\n",
      "train loss:0.5253934339106777\n",
      "train loss:0.5970556158610133\n",
      "train loss:0.4095218659444073\n",
      "train loss:0.44231802794049513\n",
      "train loss:0.45561831260961294\n",
      "train loss:0.45561425013457\n",
      "train loss:0.5667893495826284\n",
      "train loss:0.4886195727583197\n",
      "train loss:0.49955983874518445\n",
      "train loss:0.4650867086055386\n",
      "train loss:0.5542485387391423\n",
      "train loss:0.4565868274580044\n",
      "train loss:0.32931287031701784\n",
      "train loss:0.4539512087110459\n",
      "train loss:0.44989445496624475\n",
      "train loss:0.5270136347731187\n",
      "train loss:0.5851596987804203\n",
      "train loss:0.4158828202211143\n",
      "train loss:0.36406268138186104\n",
      "train loss:0.4917407111063112\n",
      "train loss:0.4058847097238843\n",
      "train loss:0.5145572088220337\n",
      "train loss:0.41591377639857596\n",
      "train loss:0.5752995500562196\n",
      "train loss:0.45256601976292876\n",
      "train loss:0.511368719328088\n",
      "train loss:0.5733265641824526\n",
      "train loss:0.4599784665267477\n",
      "train loss:0.3091247343008622\n",
      "train loss:0.6072240151068353\n",
      "train loss:0.6859420023520673\n",
      "train loss:0.5597162294878936\n",
      "train loss:0.3955168827267396\n",
      "train loss:0.45805452995448237\n",
      "train loss:0.3724972572187813\n",
      "train loss:0.408292486999475\n",
      "train loss:0.3056870959302641\n",
      "train loss:0.38016882327843476\n",
      "train loss:0.37974881269690164\n",
      "train loss:0.3004112448896717\n",
      "train loss:0.39017246494481855\n",
      "train loss:0.2805380709968789\n",
      "train loss:0.3744345877958388\n",
      "train loss:0.4662549514088804\n",
      "train loss:0.36958295075166553\n",
      "train loss:0.2573732217541041\n",
      "train loss:0.43346730185226867\n",
      "train loss:0.33931955484599535\n",
      "train loss:0.42314501726010845\n",
      "train loss:0.4459115898064595\n",
      "train loss:0.35949355336247085\n",
      "train loss:0.3071057735721421\n",
      "train loss:0.2906845354078978\n",
      "train loss:0.5572790671582137\n",
      "train loss:0.3516826261472403\n",
      "train loss:0.40460732560286006\n",
      "train loss:0.22808176171110653\n",
      "train loss:0.3957553378791222\n",
      "train loss:0.3786205213106744\n",
      "train loss:0.3775528076417443\n",
      "train loss:0.36480834494201386\n",
      "train loss:0.23446754367980746\n",
      "train loss:0.3068986831864682\n",
      "train loss:0.2873281974689225\n",
      "train loss:0.3081612549818861\n",
      "train loss:0.3218548438222703\n",
      "train loss:0.36852464248345285\n",
      "train loss:0.37087769826024763\n",
      "train loss:0.2504150927467698\n",
      "train loss:0.39473639822134293\n",
      "train loss:0.3882953817964527\n",
      "train loss:0.20962911568223128\n",
      "train loss:0.38077823675725303\n",
      "train loss:0.25122604811187005\n",
      "train loss:0.331074739837064\n",
      "train loss:0.3313141359033721\n",
      "train loss:0.41378700518929973\n",
      "train loss:0.3061069307667551\n",
      "train loss:0.3907373140203896\n",
      "train loss:0.2984139303930736\n",
      "train loss:0.3741741749803853\n",
      "train loss:0.24982186856532443\n",
      "train loss:0.4052811653203486\n",
      "train loss:0.3077233181005803\n",
      "train loss:0.2719460679135351\n",
      "train loss:0.23052114884291725\n",
      "train loss:0.30496957094321936\n",
      "train loss:0.426529689954461\n",
      "train loss:0.22546891156084992\n",
      "train loss:0.3799572085624511\n",
      "train loss:0.24629030256878376\n",
      "train loss:0.3179599267707765\n",
      "train loss:0.30859759175268053\n",
      "train loss:0.24439031647697107\n",
      "train loss:0.29427764798470446\n",
      "train loss:0.20840240153026457\n",
      "train loss:0.23049970679670423\n",
      "train loss:0.16787832814619943\n",
      "train loss:0.19321253323594795\n",
      "train loss:0.18321016525892542\n",
      "train loss:0.25987957904000325\n",
      "train loss:0.34588264117890255\n",
      "train loss:0.33448164936361685\n",
      "train loss:0.27928469074006346\n",
      "train loss:0.41966457498625426\n",
      "train loss:0.2372160725440566\n",
      "train loss:0.3059215285998074\n",
      "train loss:0.40435525342113693\n",
      "train loss:0.3593932818993263\n",
      "train loss:0.35835057963714306\n",
      "train loss:0.4429824317314589\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     16\u001b[0m network \u001b[39m=\u001b[39m SimpleConvNet(input_dim\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m28\u001b[39m,\u001b[39m28\u001b[39m), \n\u001b[1;32m     17\u001b[0m                         conv_param \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mfilter_num\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m30\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfilter_size\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpad\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstride\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m},\n\u001b[1;32m     18\u001b[0m                         hidden_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, output_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, weight_init_std\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m     20\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[1;32m     21\u001b[0m                   epochs\u001b[39m=\u001b[39mmax_epochs, mini_batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m     22\u001b[0m                   optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m'\u001b[39m, optimizer_param\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.001\u001b[39m},\n\u001b[1;32m     23\u001b[0m                   evaluate_sample_num_per_epoch\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     26\u001b[0m \u001b[39m# 매개변수 보존\u001b[39;00m\n\u001b[1;32m     27\u001b[0m network\u001b[39m.\u001b[39msave_params(\u001b[39m\"\u001b[39m\u001b[39mparams.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.ssh/pixel-font/common/trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter):\n\u001b[0;32m---> 71\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step()\n\u001b[1;32m     73\u001b[0m     test_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39maccuracy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_test, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_test)\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/.ssh/pixel-font/common/trainer.py:47\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparams, grads)\n\u001b[0;32m---> 47\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork\u001b[39m.\u001b[39;49mloss(x_batch, t_batch)\n\u001b[1;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loss_list\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtrain loss:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(loss))\n",
      "File \u001b[0;32m~/.ssh/pixel-font/common/conv_net.py:111\u001b[0m, in \u001b[0;36mSimpleConvNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, x, t):\n\u001b[0;32m--> 111\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x)\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_layer\u001b[39m.\u001b[39mforward(y, t)\n",
      "File \u001b[0;32m~/.ssh/pixel-font/common/conv_net.py:107\u001b[0m, in \u001b[0;36mSimpleConvNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    106\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m--> 107\u001b[0m         x \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.ssh/pixel-font/common/layers.py:13\u001b[0m, in \u001b[0;36mRelu.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask \u001b[39m=\u001b[39m (x \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     14\u001b[0m     out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     15\u001b[0m     out[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.conv_net import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
